{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Steps:\n",
    "# Data Collection: Gather a domain-specific dataset, potentially using web scraping or public APIs.\n",
    "# Model Preparation: Select a base LLM and prepare it for fine-tuning.\n",
    "# Fine-Tuning & Adapters: Fine-tune the model and implement the adapters.\n",
    "# Quantization: Apply quantization techniques and compare different configurations.\n",
    "# RAG Integration: Set up the vector database and integrate RAG into the system.\n",
    "# Evaluation & Testing: Design evaluation metrics, test the model, and analyze hallucinations.\n",
    "# Optimization: Experiment with prompt engineering, and explore resource-performance trade-offs.\n",
    "# Documentation: Document the process, findings, and potential improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config_dict\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemankundu_azure/anaconda3/envs/llm_assist/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# from transformers import LlamaTokenizer, LlamaForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from adapters import AdapterConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the tokenizer and model\n",
    "# model_name = \"meta-llama/Llama-2-7b\"  # Replace with the exact model path\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "# model = LlamaForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'  # model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "dataset_dict = load_from_disk(config_dict['prepare_dataset']['dataset_save_path'])\n",
    "\n",
    "# train, validation, and test sets\n",
    "train_dataset = dataset_dict['train']\n",
    "val_dataset = dataset_dict['validation']\n",
    "test_dataset = dataset_dict['test']\n",
    "\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset_tokenized = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name                                                                                           # Parameters         # Trainable     Layer Type          \n",
      "======================================================================================================================================================\n",
      "transformer.wte.weight                                                                               38597376             38597376        Parameter           \n",
      "transformer.wpe.weight                                                                               786432               786432          Parameter           \n",
      "transformer.h.0.ln_1.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.0.ln_1.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.0.attn.c_attn.weight                                                                   1769472              1769472         Parameter           \n",
      "transformer.h.0.attn.c_attn.bias                                                                     2304                 2304            Parameter           \n",
      "transformer.h.0.attn.c_proj.weight                                                                   589824               589824          Parameter           \n",
      "transformer.h.0.attn.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.h.0.ln_2.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.0.ln_2.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.0.mlp.c_fc.weight                                                                      2359296              2359296         Parameter           \n",
      "transformer.h.0.mlp.c_fc.bias                                                                        3072                 3072            Parameter           \n",
      "transformer.h.0.mlp.c_proj.weight                                                                    2359296              2359296         Parameter           \n",
      "transformer.h.0.mlp.c_proj.bias                                                                      768                  768             Parameter           \n",
      "transformer.h.1.ln_1.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.1.ln_1.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.1.attn.c_attn.weight                                                                   1769472              1769472         Parameter           \n",
      "transformer.h.1.attn.c_attn.bias                                                                     2304                 2304            Parameter           \n",
      "transformer.h.1.attn.c_proj.weight                                                                   589824               589824          Parameter           \n",
      "transformer.h.1.attn.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.h.1.ln_2.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.1.ln_2.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.1.mlp.c_fc.weight                                                                      2359296              2359296         Parameter           \n",
      "transformer.h.1.mlp.c_fc.bias                                                                        3072                 3072            Parameter           \n",
      "transformer.h.1.mlp.c_proj.weight                                                                    2359296              2359296         Parameter           \n",
      "transformer.h.1.mlp.c_proj.bias                                                                      768                  768             Parameter           \n",
      "transformer.h.2.ln_1.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.2.ln_1.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.2.attn.c_attn.weight                                                                   1769472              1769472         Parameter           \n",
      "transformer.h.2.attn.c_attn.bias                                                                     2304                 2304            Parameter           \n",
      "transformer.h.2.attn.c_proj.weight                                                                   589824               589824          Parameter           \n",
      "transformer.h.2.attn.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.h.2.ln_2.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.2.ln_2.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.2.mlp.c_fc.weight                                                                      2359296              2359296         Parameter           \n",
      "transformer.h.2.mlp.c_fc.bias                                                                        3072                 3072            Parameter           \n",
      "transformer.h.2.mlp.c_proj.weight                                                                    2359296              2359296         Parameter           \n",
      "transformer.h.2.mlp.c_proj.bias                                                                      768                  768             Parameter           \n",
      "transformer.h.3.ln_1.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.3.ln_1.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.3.attn.c_attn.weight                                                                   1769472              1769472         Parameter           \n",
      "transformer.h.3.attn.c_attn.bias                                                                     2304                 2304            Parameter           \n",
      "transformer.h.3.attn.c_proj.weight                                                                   589824               589824          Parameter           \n",
      "transformer.h.3.attn.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.h.3.ln_2.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.3.ln_2.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.3.mlp.c_fc.weight                                                                      2359296              2359296         Parameter           \n",
      "transformer.h.3.mlp.c_fc.bias                                                                        3072                 3072            Parameter           \n",
      "transformer.h.3.mlp.c_proj.weight                                                                    2359296              2359296         Parameter           \n",
      "transformer.h.3.mlp.c_proj.bias                                                                      768                  768             Parameter           \n",
      "transformer.h.4.ln_1.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.4.ln_1.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.4.attn.c_attn.weight                                                                   1769472              1769472         Parameter           \n",
      "transformer.h.4.attn.c_attn.bias                                                                     2304                 2304            Parameter           \n",
      "transformer.h.4.attn.c_proj.weight                                                                   589824               589824          Parameter           \n",
      "transformer.h.4.attn.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.h.4.ln_2.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.4.ln_2.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.4.mlp.c_fc.weight                                                                      2359296              2359296         Parameter           \n",
      "transformer.h.4.mlp.c_fc.bias                                                                        3072                 3072            Parameter           \n",
      "transformer.h.4.mlp.c_proj.weight                                                                    2359296              2359296         Parameter           \n",
      "transformer.h.4.mlp.c_proj.bias                                                                      768                  768             Parameter           \n",
      "transformer.h.5.ln_1.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.5.ln_1.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.5.attn.c_attn.weight                                                                   1769472              1769472         Parameter           \n",
      "transformer.h.5.attn.c_attn.bias                                                                     2304                 2304            Parameter           \n",
      "transformer.h.5.attn.c_proj.weight                                                                   589824               589824          Parameter           \n",
      "transformer.h.5.attn.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.h.5.ln_2.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.5.ln_2.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.5.mlp.c_fc.weight                                                                      2359296              2359296         Parameter           \n",
      "transformer.h.5.mlp.c_fc.bias                                                                        3072                 3072            Parameter           \n",
      "transformer.h.5.mlp.c_proj.weight                                                                    2359296              2359296         Parameter           \n",
      "transformer.h.5.mlp.c_proj.bias                                                                      768                  768             Parameter           \n",
      "transformer.h.6.ln_1.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.6.ln_1.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.6.attn.c_attn.weight                                                                   1769472              1769472         Parameter           \n",
      "transformer.h.6.attn.c_attn.bias                                                                     2304                 2304            Parameter           \n",
      "transformer.h.6.attn.c_proj.weight                                                                   589824               589824          Parameter           \n",
      "transformer.h.6.attn.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.h.6.ln_2.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.6.ln_2.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.6.mlp.c_fc.weight                                                                      2359296              2359296         Parameter           \n",
      "transformer.h.6.mlp.c_fc.bias                                                                        3072                 3072            Parameter           \n",
      "transformer.h.6.mlp.c_proj.weight                                                                    2359296              2359296         Parameter           \n",
      "transformer.h.6.mlp.c_proj.bias                                                                      768                  768             Parameter           \n",
      "transformer.h.7.ln_1.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.7.ln_1.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.7.attn.c_attn.weight                                                                   1769472              1769472         Parameter           \n",
      "transformer.h.7.attn.c_attn.bias                                                                     2304                 2304            Parameter           \n",
      "transformer.h.7.attn.c_proj.weight                                                                   589824               589824          Parameter           \n",
      "transformer.h.7.attn.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.h.7.ln_2.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.7.ln_2.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.7.mlp.c_fc.weight                                                                      2359296              2359296         Parameter           \n",
      "transformer.h.7.mlp.c_fc.bias                                                                        3072                 3072            Parameter           \n",
      "transformer.h.7.mlp.c_proj.weight                                                                    2359296              2359296         Parameter           \n",
      "transformer.h.7.mlp.c_proj.bias                                                                      768                  768             Parameter           \n",
      "transformer.h.8.ln_1.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.8.ln_1.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.8.attn.c_attn.weight                                                                   1769472              1769472         Parameter           \n",
      "transformer.h.8.attn.c_attn.bias                                                                     2304                 2304            Parameter           \n",
      "transformer.h.8.attn.c_proj.weight                                                                   589824               589824          Parameter           \n",
      "transformer.h.8.attn.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.h.8.ln_2.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.8.ln_2.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.8.mlp.c_fc.weight                                                                      2359296              2359296         Parameter           \n",
      "transformer.h.8.mlp.c_fc.bias                                                                        3072                 3072            Parameter           \n",
      "transformer.h.8.mlp.c_proj.weight                                                                    2359296              2359296         Parameter           \n",
      "transformer.h.8.mlp.c_proj.bias                                                                      768                  768             Parameter           \n",
      "transformer.h.9.ln_1.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.9.ln_1.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.9.attn.c_attn.weight                                                                   1769472              1769472         Parameter           \n",
      "transformer.h.9.attn.c_attn.bias                                                                     2304                 2304            Parameter           \n",
      "transformer.h.9.attn.c_proj.weight                                                                   589824               589824          Parameter           \n",
      "transformer.h.9.attn.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.h.9.ln_2.weight                                                                          768                  768             Parameter           \n",
      "transformer.h.9.ln_2.bias                                                                            768                  768             Parameter           \n",
      "transformer.h.9.mlp.c_fc.weight                                                                      2359296              2359296         Parameter           \n",
      "transformer.h.9.mlp.c_fc.bias                                                                        3072                 3072            Parameter           \n",
      "transformer.h.9.mlp.c_proj.weight                                                                    2359296              2359296         Parameter           \n",
      "transformer.h.9.mlp.c_proj.bias                                                                      768                  768             Parameter           \n",
      "transformer.h.10.ln_1.weight                                                                         768                  768             Parameter           \n",
      "transformer.h.10.ln_1.bias                                                                           768                  768             Parameter           \n",
      "transformer.h.10.attn.c_attn.weight                                                                  1769472              1769472         Parameter           \n",
      "transformer.h.10.attn.c_attn.bias                                                                    2304                 2304            Parameter           \n",
      "transformer.h.10.attn.c_proj.weight                                                                  589824               589824          Parameter           \n",
      "transformer.h.10.attn.c_proj.bias                                                                    768                  768             Parameter           \n",
      "transformer.h.10.ln_2.weight                                                                         768                  768             Parameter           \n",
      "transformer.h.10.ln_2.bias                                                                           768                  768             Parameter           \n",
      "transformer.h.10.mlp.c_fc.weight                                                                     2359296              2359296         Parameter           \n",
      "transformer.h.10.mlp.c_fc.bias                                                                       3072                 3072            Parameter           \n",
      "transformer.h.10.mlp.c_proj.weight                                                                   2359296              2359296         Parameter           \n",
      "transformer.h.10.mlp.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.h.11.ln_1.weight                                                                         768                  768             Parameter           \n",
      "transformer.h.11.ln_1.bias                                                                           768                  768             Parameter           \n",
      "transformer.h.11.attn.c_attn.weight                                                                  1769472              1769472         Parameter           \n",
      "transformer.h.11.attn.c_attn.bias                                                                    2304                 2304            Parameter           \n",
      "transformer.h.11.attn.c_proj.weight                                                                  589824               589824          Parameter           \n",
      "transformer.h.11.attn.c_proj.bias                                                                    768                  768             Parameter           \n",
      "transformer.h.11.ln_2.weight                                                                         768                  768             Parameter           \n",
      "transformer.h.11.ln_2.bias                                                                           768                  768             Parameter           \n",
      "transformer.h.11.mlp.c_fc.weight                                                                     2359296              2359296         Parameter           \n",
      "transformer.h.11.mlp.c_fc.bias                                                                       3072                 3072            Parameter           \n",
      "transformer.h.11.mlp.c_proj.weight                                                                   2359296              2359296         Parameter           \n",
      "transformer.h.11.mlp.c_proj.bias                                                                     768                  768             Parameter           \n",
      "transformer.ln_f.weight                                                                              768                  768             Parameter           \n",
      "transformer.ln_f.bias                                                                                768                  768             Parameter           \n",
      "======================================================================================================================================================\n",
      "Total Parameters: 124439808\n",
      "Total Trainable Parameters: 124439808\n",
      "\n",
      "Memory Requirements (in GB):\n",
      "Parameters Memory: 0.4636 GB\n",
      "Gradients Memory: 0.4636 GB\n",
      "Optimizer Memory: 0.9271 GB\n",
      "Activations Memory: 0.0117 GB\n",
      "Total Estimated Memory: 1.8660 GB\n"
     ]
    }
   ],
   "source": [
    "tools.print_model_parameters_and_memory(model, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemankundu_azure/anaconda3/envs/llm_assist/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2SdpaAttention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  \n",
    "    lora_alpha=16,  \n",
    "    lora_dropout=0.1,  \n",
    "    target_modules=[\"c_attn\", \"c_fc\", \"c_proj\"],  # Target GPT-2's attention and MLP layers\n",
    "    task_type=TaskType.CAUSAL_LM,  # Task type: Causal Language Modeling\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name                                                                                           # Parameters         # Trainable     Layer Type          \n",
      "======================================================================================================================================================\n",
      "base_model.model.transformer.wte.weight                                                              38597376             0               Parameter           \n",
      "base_model.model.transformer.wpe.weight                                                              786432               0               Parameter           \n",
      "base_model.model.transformer.h.0.ln_1.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.0.ln_1.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.0.attn.c_attn.base_layer.weight                                       1769472              0               Parameter           \n",
      "base_model.model.transformer.h.0.attn.c_attn.base_layer.bias                                         2304                 0               Parameter           \n",
      "base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight                                   18432                18432           Parameter           \n",
      "base_model.model.transformer.h.0.attn.c_proj.base_layer.weight                                       589824               0               Parameter           \n",
      "base_model.model.transformer.h.0.attn.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.0.ln_2.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.0.ln_2.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.0.mlp.c_fc.base_layer.weight                                          2359296              0               Parameter           \n",
      "base_model.model.transformer.h.0.mlp.c_fc.base_layer.bias                                            3072                 0               Parameter           \n",
      "base_model.model.transformer.h.0.mlp.c_fc.lora_A.default.weight                                      6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.0.mlp.c_fc.lora_B.default.weight                                      24576                24576           Parameter           \n",
      "base_model.model.transformer.h.0.mlp.c_proj.base_layer.weight                                        2359296              0               Parameter           \n",
      "base_model.model.transformer.h.0.mlp.c_proj.base_layer.bias                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight                                    24576                24576           Parameter           \n",
      "base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight                                    6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.1.ln_1.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.1.ln_1.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.1.attn.c_attn.base_layer.weight                                       1769472              0               Parameter           \n",
      "base_model.model.transformer.h.1.attn.c_attn.base_layer.bias                                         2304                 0               Parameter           \n",
      "base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight                                   18432                18432           Parameter           \n",
      "base_model.model.transformer.h.1.attn.c_proj.base_layer.weight                                       589824               0               Parameter           \n",
      "base_model.model.transformer.h.1.attn.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.1.attn.c_proj.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.1.attn.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.1.ln_2.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.1.ln_2.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.1.mlp.c_fc.base_layer.weight                                          2359296              0               Parameter           \n",
      "base_model.model.transformer.h.1.mlp.c_fc.base_layer.bias                                            3072                 0               Parameter           \n",
      "base_model.model.transformer.h.1.mlp.c_fc.lora_A.default.weight                                      6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.1.mlp.c_fc.lora_B.default.weight                                      24576                24576           Parameter           \n",
      "base_model.model.transformer.h.1.mlp.c_proj.base_layer.weight                                        2359296              0               Parameter           \n",
      "base_model.model.transformer.h.1.mlp.c_proj.base_layer.bias                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.1.mlp.c_proj.lora_A.default.weight                                    24576                24576           Parameter           \n",
      "base_model.model.transformer.h.1.mlp.c_proj.lora_B.default.weight                                    6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.2.ln_1.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.2.ln_1.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.2.attn.c_attn.base_layer.weight                                       1769472              0               Parameter           \n",
      "base_model.model.transformer.h.2.attn.c_attn.base_layer.bias                                         2304                 0               Parameter           \n",
      "base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight                                   18432                18432           Parameter           \n",
      "base_model.model.transformer.h.2.attn.c_proj.base_layer.weight                                       589824               0               Parameter           \n",
      "base_model.model.transformer.h.2.attn.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.2.attn.c_proj.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.2.attn.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.2.ln_2.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.2.ln_2.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.2.mlp.c_fc.base_layer.weight                                          2359296              0               Parameter           \n",
      "base_model.model.transformer.h.2.mlp.c_fc.base_layer.bias                                            3072                 0               Parameter           \n",
      "base_model.model.transformer.h.2.mlp.c_fc.lora_A.default.weight                                      6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.2.mlp.c_fc.lora_B.default.weight                                      24576                24576           Parameter           \n",
      "base_model.model.transformer.h.2.mlp.c_proj.base_layer.weight                                        2359296              0               Parameter           \n",
      "base_model.model.transformer.h.2.mlp.c_proj.base_layer.bias                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.2.mlp.c_proj.lora_A.default.weight                                    24576                24576           Parameter           \n",
      "base_model.model.transformer.h.2.mlp.c_proj.lora_B.default.weight                                    6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.3.ln_1.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.3.ln_1.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.3.attn.c_attn.base_layer.weight                                       1769472              0               Parameter           \n",
      "base_model.model.transformer.h.3.attn.c_attn.base_layer.bias                                         2304                 0               Parameter           \n",
      "base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight                                   18432                18432           Parameter           \n",
      "base_model.model.transformer.h.3.attn.c_proj.base_layer.weight                                       589824               0               Parameter           \n",
      "base_model.model.transformer.h.3.attn.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.3.attn.c_proj.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.3.attn.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.3.ln_2.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.3.ln_2.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.3.mlp.c_fc.base_layer.weight                                          2359296              0               Parameter           \n",
      "base_model.model.transformer.h.3.mlp.c_fc.base_layer.bias                                            3072                 0               Parameter           \n",
      "base_model.model.transformer.h.3.mlp.c_fc.lora_A.default.weight                                      6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.3.mlp.c_fc.lora_B.default.weight                                      24576                24576           Parameter           \n",
      "base_model.model.transformer.h.3.mlp.c_proj.base_layer.weight                                        2359296              0               Parameter           \n",
      "base_model.model.transformer.h.3.mlp.c_proj.base_layer.bias                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.3.mlp.c_proj.lora_A.default.weight                                    24576                24576           Parameter           \n",
      "base_model.model.transformer.h.3.mlp.c_proj.lora_B.default.weight                                    6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.4.ln_1.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.4.ln_1.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.4.attn.c_attn.base_layer.weight                                       1769472              0               Parameter           \n",
      "base_model.model.transformer.h.4.attn.c_attn.base_layer.bias                                         2304                 0               Parameter           \n",
      "base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight                                   18432                18432           Parameter           \n",
      "base_model.model.transformer.h.4.attn.c_proj.base_layer.weight                                       589824               0               Parameter           \n",
      "base_model.model.transformer.h.4.attn.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.4.attn.c_proj.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.4.attn.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.4.ln_2.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.4.ln_2.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.4.mlp.c_fc.base_layer.weight                                          2359296              0               Parameter           \n",
      "base_model.model.transformer.h.4.mlp.c_fc.base_layer.bias                                            3072                 0               Parameter           \n",
      "base_model.model.transformer.h.4.mlp.c_fc.lora_A.default.weight                                      6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.4.mlp.c_fc.lora_B.default.weight                                      24576                24576           Parameter           \n",
      "base_model.model.transformer.h.4.mlp.c_proj.base_layer.weight                                        2359296              0               Parameter           \n",
      "base_model.model.transformer.h.4.mlp.c_proj.base_layer.bias                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.4.mlp.c_proj.lora_A.default.weight                                    24576                24576           Parameter           \n",
      "base_model.model.transformer.h.4.mlp.c_proj.lora_B.default.weight                                    6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.5.ln_1.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.5.ln_1.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.5.attn.c_attn.base_layer.weight                                       1769472              0               Parameter           \n",
      "base_model.model.transformer.h.5.attn.c_attn.base_layer.bias                                         2304                 0               Parameter           \n",
      "base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight                                   18432                18432           Parameter           \n",
      "base_model.model.transformer.h.5.attn.c_proj.base_layer.weight                                       589824               0               Parameter           \n",
      "base_model.model.transformer.h.5.attn.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.5.attn.c_proj.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.5.attn.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.5.ln_2.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.5.ln_2.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.5.mlp.c_fc.base_layer.weight                                          2359296              0               Parameter           \n",
      "base_model.model.transformer.h.5.mlp.c_fc.base_layer.bias                                            3072                 0               Parameter           \n",
      "base_model.model.transformer.h.5.mlp.c_fc.lora_A.default.weight                                      6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.5.mlp.c_fc.lora_B.default.weight                                      24576                24576           Parameter           \n",
      "base_model.model.transformer.h.5.mlp.c_proj.base_layer.weight                                        2359296              0               Parameter           \n",
      "base_model.model.transformer.h.5.mlp.c_proj.base_layer.bias                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.5.mlp.c_proj.lora_A.default.weight                                    24576                24576           Parameter           \n",
      "base_model.model.transformer.h.5.mlp.c_proj.lora_B.default.weight                                    6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.6.ln_1.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.6.ln_1.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.6.attn.c_attn.base_layer.weight                                       1769472              0               Parameter           \n",
      "base_model.model.transformer.h.6.attn.c_attn.base_layer.bias                                         2304                 0               Parameter           \n",
      "base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight                                   18432                18432           Parameter           \n",
      "base_model.model.transformer.h.6.attn.c_proj.base_layer.weight                                       589824               0               Parameter           \n",
      "base_model.model.transformer.h.6.attn.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.6.attn.c_proj.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.6.attn.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.6.ln_2.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.6.ln_2.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.6.mlp.c_fc.base_layer.weight                                          2359296              0               Parameter           \n",
      "base_model.model.transformer.h.6.mlp.c_fc.base_layer.bias                                            3072                 0               Parameter           \n",
      "base_model.model.transformer.h.6.mlp.c_fc.lora_A.default.weight                                      6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.6.mlp.c_fc.lora_B.default.weight                                      24576                24576           Parameter           \n",
      "base_model.model.transformer.h.6.mlp.c_proj.base_layer.weight                                        2359296              0               Parameter           \n",
      "base_model.model.transformer.h.6.mlp.c_proj.base_layer.bias                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.6.mlp.c_proj.lora_A.default.weight                                    24576                24576           Parameter           \n",
      "base_model.model.transformer.h.6.mlp.c_proj.lora_B.default.weight                                    6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.7.ln_1.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.7.ln_1.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.7.attn.c_attn.base_layer.weight                                       1769472              0               Parameter           \n",
      "base_model.model.transformer.h.7.attn.c_attn.base_layer.bias                                         2304                 0               Parameter           \n",
      "base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight                                   18432                18432           Parameter           \n",
      "base_model.model.transformer.h.7.attn.c_proj.base_layer.weight                                       589824               0               Parameter           \n",
      "base_model.model.transformer.h.7.attn.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.7.attn.c_proj.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.7.attn.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.7.ln_2.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.7.ln_2.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.7.mlp.c_fc.base_layer.weight                                          2359296              0               Parameter           \n",
      "base_model.model.transformer.h.7.mlp.c_fc.base_layer.bias                                            3072                 0               Parameter           \n",
      "base_model.model.transformer.h.7.mlp.c_fc.lora_A.default.weight                                      6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.7.mlp.c_fc.lora_B.default.weight                                      24576                24576           Parameter           \n",
      "base_model.model.transformer.h.7.mlp.c_proj.base_layer.weight                                        2359296              0               Parameter           \n",
      "base_model.model.transformer.h.7.mlp.c_proj.base_layer.bias                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.7.mlp.c_proj.lora_A.default.weight                                    24576                24576           Parameter           \n",
      "base_model.model.transformer.h.7.mlp.c_proj.lora_B.default.weight                                    6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.8.ln_1.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.8.ln_1.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.8.attn.c_attn.base_layer.weight                                       1769472              0               Parameter           \n",
      "base_model.model.transformer.h.8.attn.c_attn.base_layer.bias                                         2304                 0               Parameter           \n",
      "base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight                                   18432                18432           Parameter           \n",
      "base_model.model.transformer.h.8.attn.c_proj.base_layer.weight                                       589824               0               Parameter           \n",
      "base_model.model.transformer.h.8.attn.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.8.attn.c_proj.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.8.attn.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.8.ln_2.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.8.ln_2.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.8.mlp.c_fc.base_layer.weight                                          2359296              0               Parameter           \n",
      "base_model.model.transformer.h.8.mlp.c_fc.base_layer.bias                                            3072                 0               Parameter           \n",
      "base_model.model.transformer.h.8.mlp.c_fc.lora_A.default.weight                                      6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.8.mlp.c_fc.lora_B.default.weight                                      24576                24576           Parameter           \n",
      "base_model.model.transformer.h.8.mlp.c_proj.base_layer.weight                                        2359296              0               Parameter           \n",
      "base_model.model.transformer.h.8.mlp.c_proj.base_layer.bias                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.8.mlp.c_proj.lora_A.default.weight                                    24576                24576           Parameter           \n",
      "base_model.model.transformer.h.8.mlp.c_proj.lora_B.default.weight                                    6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.9.ln_1.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.9.ln_1.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.9.attn.c_attn.base_layer.weight                                       1769472              0               Parameter           \n",
      "base_model.model.transformer.h.9.attn.c_attn.base_layer.bias                                         2304                 0               Parameter           \n",
      "base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight                                   18432                18432           Parameter           \n",
      "base_model.model.transformer.h.9.attn.c_proj.base_layer.weight                                       589824               0               Parameter           \n",
      "base_model.model.transformer.h.9.attn.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.9.attn.c_proj.lora_A.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.9.attn.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.9.ln_2.weight                                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.9.ln_2.bias                                                           768                  0               Parameter           \n",
      "base_model.model.transformer.h.9.mlp.c_fc.base_layer.weight                                          2359296              0               Parameter           \n",
      "base_model.model.transformer.h.9.mlp.c_fc.base_layer.bias                                            3072                 0               Parameter           \n",
      "base_model.model.transformer.h.9.mlp.c_fc.lora_A.default.weight                                      6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.9.mlp.c_fc.lora_B.default.weight                                      24576                24576           Parameter           \n",
      "base_model.model.transformer.h.9.mlp.c_proj.base_layer.weight                                        2359296              0               Parameter           \n",
      "base_model.model.transformer.h.9.mlp.c_proj.base_layer.bias                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.9.mlp.c_proj.lora_A.default.weight                                    24576                24576           Parameter           \n",
      "base_model.model.transformer.h.9.mlp.c_proj.lora_B.default.weight                                    6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.10.ln_1.weight                                                        768                  0               Parameter           \n",
      "base_model.model.transformer.h.10.ln_1.bias                                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.10.attn.c_attn.base_layer.weight                                      1769472              0               Parameter           \n",
      "base_model.model.transformer.h.10.attn.c_attn.base_layer.bias                                        2304                 0               Parameter           \n",
      "base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight                                  6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight                                  18432                18432           Parameter           \n",
      "base_model.model.transformer.h.10.attn.c_proj.base_layer.weight                                      589824               0               Parameter           \n",
      "base_model.model.transformer.h.10.attn.c_proj.base_layer.bias                                        768                  0               Parameter           \n",
      "base_model.model.transformer.h.10.attn.c_proj.lora_A.default.weight                                  6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.10.attn.c_proj.lora_B.default.weight                                  6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.10.ln_2.weight                                                        768                  0               Parameter           \n",
      "base_model.model.transformer.h.10.ln_2.bias                                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.10.mlp.c_fc.base_layer.weight                                         2359296              0               Parameter           \n",
      "base_model.model.transformer.h.10.mlp.c_fc.base_layer.bias                                           3072                 0               Parameter           \n",
      "base_model.model.transformer.h.10.mlp.c_fc.lora_A.default.weight                                     6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.10.mlp.c_fc.lora_B.default.weight                                     24576                24576           Parameter           \n",
      "base_model.model.transformer.h.10.mlp.c_proj.base_layer.weight                                       2359296              0               Parameter           \n",
      "base_model.model.transformer.h.10.mlp.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.10.mlp.c_proj.lora_A.default.weight                                   24576                24576           Parameter           \n",
      "base_model.model.transformer.h.10.mlp.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.11.ln_1.weight                                                        768                  0               Parameter           \n",
      "base_model.model.transformer.h.11.ln_1.bias                                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.11.attn.c_attn.base_layer.weight                                      1769472              0               Parameter           \n",
      "base_model.model.transformer.h.11.attn.c_attn.base_layer.bias                                        2304                 0               Parameter           \n",
      "base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight                                  6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight                                  18432                18432           Parameter           \n",
      "base_model.model.transformer.h.11.attn.c_proj.base_layer.weight                                      589824               0               Parameter           \n",
      "base_model.model.transformer.h.11.attn.c_proj.base_layer.bias                                        768                  0               Parameter           \n",
      "base_model.model.transformer.h.11.attn.c_proj.lora_A.default.weight                                  6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.11.attn.c_proj.lora_B.default.weight                                  6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.11.ln_2.weight                                                        768                  0               Parameter           \n",
      "base_model.model.transformer.h.11.ln_2.bias                                                          768                  0               Parameter           \n",
      "base_model.model.transformer.h.11.mlp.c_fc.base_layer.weight                                         2359296              0               Parameter           \n",
      "base_model.model.transformer.h.11.mlp.c_fc.base_layer.bias                                           3072                 0               Parameter           \n",
      "base_model.model.transformer.h.11.mlp.c_fc.lora_A.default.weight                                     6144                 6144            Parameter           \n",
      "base_model.model.transformer.h.11.mlp.c_fc.lora_B.default.weight                                     24576                24576           Parameter           \n",
      "base_model.model.transformer.h.11.mlp.c_proj.base_layer.weight                                       2359296              0               Parameter           \n",
      "base_model.model.transformer.h.11.mlp.c_proj.base_layer.bias                                         768                  0               Parameter           \n",
      "base_model.model.transformer.h.11.mlp.c_proj.lora_A.default.weight                                   24576                24576           Parameter           \n",
      "base_model.model.transformer.h.11.mlp.c_proj.lora_B.default.weight                                   6144                 6144            Parameter           \n",
      "base_model.model.transformer.ln_f.weight                                                             768                  0               Parameter           \n",
      "base_model.model.transformer.ln_f.bias                                                               768                  0               Parameter           \n",
      "======================================================================================================================================================\n",
      "Total Parameters: 125619456\n",
      "Total Trainable Parameters: 1179648\n",
      "\n",
      "Memory Requirements (in GB):\n",
      "Parameters Memory: 0.4680 GB\n",
      "Gradients Memory: 0.0044 GB\n",
      "Optimizer Memory: 0.0088 GB\n",
      "Activations Memory: 0.0117 GB\n",
      "Total Estimated Memory: 0.4929 GB\n"
     ]
    }
   ],
   "source": [
    "tools.print_model_parameters_and_memory(model, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bitsandbytes as bnb\n",
    "\n",
    "# # Example of quantizing specific layers to 4-bit\n",
    "# # model = model.to('cuda')  # Ensure the model is on GPU if available\n",
    "\n",
    "# # Apply 4-bit quantization to the model\n",
    "# for name, module in model.named_modules():\n",
    "#     if any(target in name for target in [\"c_attn\", \"c_fc\", \"c_proj\"]):\n",
    "#         quantized_module = bnb.nn.Int8Params(module.weight, requires_grad=True)\n",
    "#         module.weight = quantized_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configure Adapter\n",
    "# adapter_config = AdapterConfig(\n",
    "#     input_dim=model.config.hidden_size,\n",
    "#     output_dim=model.config.hidden_size,\n",
    "#     adapter_dim=512,  # Dimension of the adapter layer\n",
    "#     activation=\"relu\"  # Activation function for the adapter\n",
    "# )\n",
    "# model.add_adapter(\"my_adapter\", adapter_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    eval_strategy='steps',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=val_dataset_tokenized,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 53:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.086200</td>\n",
       "      <td>2.754037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.764700</td>\n",
       "      <td>2.746497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.498900</td>\n",
       "      <td>2.737292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.892700</td>\n",
       "      <td>2.729145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.805500</td>\n",
       "      <td>2.723517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.340000</td>\n",
       "      <td>2.721076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.985900</td>\n",
       "      <td>2.717703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.121700</td>\n",
       "      <td>2.714619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.190900</td>\n",
       "      <td>2.709363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.411800</td>\n",
       "      <td>2.705761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.296300</td>\n",
       "      <td>2.701714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.150800</td>\n",
       "      <td>2.695239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.466000</td>\n",
       "      <td>2.689502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.770700</td>\n",
       "      <td>2.686311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.588500</td>\n",
       "      <td>2.684268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.945000</td>\n",
       "      <td>2.682315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.661400</td>\n",
       "      <td>2.680687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.363900</td>\n",
       "      <td>2.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.563600</td>\n",
       "      <td>2.677066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.773100</td>\n",
       "      <td>2.676221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.826400</td>\n",
       "      <td>2.675432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.913000</td>\n",
       "      <td>2.674807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.858200</td>\n",
       "      <td>2.674189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.287300</td>\n",
       "      <td>2.673585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.510600</td>\n",
       "      <td>2.673361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "No columns in the dataset match the model's forward method signature. The following columns have been ignored: [text]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate on the test set\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_results)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_assist/lib/python3.12/site-packages/transformers/trainer.py:3659\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 3659\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   3661\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_assist/lib/python3.12/site-packages/transformers/trainer.py:967\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m    964\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m--> 967\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remove_unused_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_collator_with_removed_columns(data_collator, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_assist/lib/python3.12/site-packages/transformers/trainer.py:820\u001b[0m, in \u001b[0;36mTrainer._remove_unused_columns\u001b[0;34m(self, dataset, description)\u001b[0m\n\u001b[1;32m    818\u001b[0m columns \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature_columns \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcolumn_names]\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo columns in the dataset match the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms forward method signature. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following columns have been ignored: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ignored_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m     )\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(datasets\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    827\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mset_format(\n\u001b[1;32m    828\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m], columns\u001b[38;5;241m=\u001b[39mcolumns, format_kwargs\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    829\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [text]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.7191321849823, 'eval_runtime': 37.1388, 'eval_samples_per_second': 0.296, 'eval_steps_per_second': 0.054, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(test_dataset_tokenized)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "tokenizer.save_pretrained('./fine-tuned-model-lora')\n",
    "# model.save_adapter('./my_adapter', \"my_adapter\")\n",
    "model.save_pretrained('./fine-tuned-model-lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_assist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
